{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from keybert import KeyBERT\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM\n",
    "import transformers\n",
    "# transformers.logging.set_verbosity_error()\n",
    "\n",
    "import time\n",
    "\n",
    "import requests\n",
    "from newspaper import Article\n",
    "import spacy\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Dante_Alighieri'\n",
    "\n",
    "article = Article(url)\n",
    "article.download()\n",
    "article.parse()\n",
    "len(article.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model token config\n",
    "tokenizer_checkpoint = 'facebook/bart-large-mnli'\n",
    "model_checkpoint = 'facebook/bart-large-cnn'\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = \"VDT/Addetti ad AttivitÃ  AttivitÕ di Ufficio Turnisti - DT, à\"\n",
    "# s.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_nest_sentences(document: str, token_max_length = 1024):\n",
    "  sents = []\n",
    "  length = 0\n",
    "  tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "  doc = nlp(document)\n",
    "  s = ''\n",
    "  for sentence in doc.sents:\n",
    "    tokens_in_sentence = tokenizer(str(sentence), truncation=False, padding=False)[0]\n",
    "    length += len(tokens_in_sentence) # how many tokens the current sentence have summed to the previous\n",
    "    if length < token_max_length:\n",
    "      s += sentence.text\n",
    "    else:\n",
    "      sents.append(s)\n",
    "      tokens_in_sentence = tokenizer(str(s), truncation=False, padding=False)[0]\n",
    "      s = sentence.text\n",
    "      length = 0\n",
    "  sents.append(s) # be sure to append even string with less number of tokens than the maximum one\n",
    "  return sents\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer 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\"}\n",
    "API_URL = \"https://api-inference.huggingface.co/models/facebook/bart-large-cnn\"\n",
    "\n",
    "def download_text(url: str):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return article\n",
    "\n",
    "def get_hf_inference_data_input(article_text):\n",
    "    payload = {'inputs': article_text, 'parameters': {'do_sample': False}}\n",
    "    data = json.dumps(payload)\n",
    "    return data\n",
    "\n",
    "\n",
    "def generate_summary(url: str):\n",
    "    article = download_text(url)\n",
    "    data = get_hf_inference_data_input(article.text)\n",
    "    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "    summary = json.loads(response.content.decode(\"utf-8\"))\n",
    "    summary = summary[0]['summary_text']\n",
    "    return summary\n",
    "\n",
    "def generate_msummary(text: str):\n",
    "    data = get_hf_inference_data_input(text)\n",
    "    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "    summary = json.loads(response.content.decode(\"utf-8\"))\n",
    "    summary = summary[0]['summary_text']\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(article.text)\n",
    "len(list(doc.sents))\n",
    "for sentence in doc.sents:\n",
    "    s = tokenizer(str(sentence), truncation=False, padding=False, return_tensors='pt')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = my_nest_sentences(article.text)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelInference:\n",
    "    def __init__(self, tokenizer_checkpoint, model_checkpoint, quantize=False):\n",
    "        torch.set_num_threads(1)\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "        if quantize:\n",
    "            model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "        \n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, msg: str):\n",
    "        with torch.no_grad():\n",
    "            inputs = self.tokenizer(str(msg), max_length_data=1024, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested = my_nest_sentences(article.text)\n",
    "chunks = [x for x in nested]\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "for chunk in chunks:\n",
    "    summary = generate_msummary(chunk)\n",
    "    time.sleep(1)\n",
    "    print(summary)\n",
    "    summaries.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keybert = KeyBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_summary = ''.join(summaries)\n",
    "keywords = keybert.extract_keywords(\n",
    "  total_summary, \n",
    "  keyphrase_ngram_range=(1, 1),\n",
    "  stop_words='english', \n",
    "  use_mmr=True, \n",
    "  diversity=0.5,\n",
    "  top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_labels = ['literature', 'cooking', 'dancing', 'exploration', 'finance', 'technology', 'science', 'programming']\n",
    "k = classifier(total_summary, candidate_labels, multi_label=True)\n",
    "k\n",
    "# setting candidate labels to interested topics and allowing the research of a summary by the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "pattern = [{\"TEXT\": 'Alcuni'}, {\"TEXT\": 'versi'}, {\"TEXT\": 'del'}, {\"TEXT\": \"Paradiso\"}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('SEGNO_PATTERN', [pattern])\n",
    "matches = matcher(doc)\n",
    "for match_id,start,end in matches:\n",
    "    print(doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "for chunk in chunks:\n",
    "    summary = generate_msummary(chunk)\n",
    "    print(summary)\n",
    "    summaries.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary = ''.join(summaries)\n",
    "len(article.text)\n",
    "len(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new_file.txt', '+w') as f:\n",
    "    f.write(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ebb7f5bbb98572b15bff58c8aed07b0a474ef83fd7734f6b950989757680d4c6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venvML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
