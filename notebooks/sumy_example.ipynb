{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "from transformers import BartForConditionalGeneration, BartTokenizerFast, BertForSequenceClassification, BertTokenizerFast, pipeline\n",
    "\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_summary = \"facebook/bart-large-cnn\"\n",
    "# checkpoint_summary = \"sshleifer/distilbart-xsum-12-6\"\n",
    "checkpoint_sentiment = \"yiyanghkust/finbert-tone\"\n",
    "tokenizer = BartTokenizerFast.from_pretrained(checkpoint_summary)\n",
    "model = BartForConditionalGeneration.from_pretrained(checkpoint_summary)\n",
    "\n",
    "finbert = BertForSequenceClassification.from_pretrained(checkpoint_sentiment, num_labels=3)\n",
    "tokenizer_sentiment = BertTokenizerFast.from_pretrained(checkpoint_sentiment)\n",
    "nlp_sentiment = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extractive_summary_from_url(url: str, language: str, sentence_count: int) -> str:\n",
    "  parser = HtmlParser.from_url(url, Tokenizer(language))\n",
    "  stemmer = Stemmer(language)\n",
    "  summarizer = Summarizer(stemmer)\n",
    "  summarizer.stop_words = get_stop_words(language)\n",
    "  extractive_summary_from_url = ' '.join([sent._text for sent in summarizer(parser.document, sentence_count)])\n",
    "  return extractive_summary_from_url\n",
    "\n",
    "\n",
    "def get_summary(dict_summarizer_model, dict_tokenizer, text_content):\n",
    "  # text_content = get_extractive_summary(text_content, EXTRACTED_ARTICLE_SENTENCES_LEN)\n",
    "  tokenizer = dict_tokenizer['tokenizer']\n",
    "  model = dict_summarizer_model['model']\n",
    "\n",
    "  inputs = tokenizer(text_content, max_length=dict_tokenizer['max_length'], truncation=True, return_tensors=\"pt\")\n",
    "  outputs = model.generate(\n",
    "      inputs[\"input_ids\"], max_length=dict_summarizer_model['max_length'], min_length=dict_summarizer_model['min_length'], \n",
    "  )\n",
    "\n",
    "  summarized_text = tokenizer.decode(outputs[0])\n",
    "  match = re.search(r\"<s>(.*)</s>\", summarized_text)\n",
    "  if match is not None: summarized_text = match.group(1)\n",
    "\n",
    "  return summarized_text.replace('<s>', '').replace('</s>', '') \n",
    "  \n",
    "\n",
    "model_dict = {\n",
    "  'model': model, \n",
    "  'max_length': 512,\n",
    "  'min_length': 120\n",
    "}\n",
    "\n",
    "tokenizer_dict = {\n",
    "  'tokenizer': tokenizer, \n",
    "  'max_length': 1024\n",
    "}\n",
    "\n",
    "LANGUAGE = \"english\"\n",
    "SENTENCE_COUNT = 15\n",
    "\n",
    "url = \"https://news.iobanker.com/2022/07/01/bitcoin-will-see-long-bear-market-says-trader-with-btc-price-stuck-at-19k/\"\n",
    "\n",
    "extractive_summary_from_url = get_extractive_summary_from_url(url, LANGUAGE, SENTENCE_COUNT)\n",
    "display(Markdown('### Extractive summary:'))\n",
    "print(extractive_summary_from_url)\n",
    "print()\n",
    "display(Markdown('### Abstractive summary:'))\n",
    "abstractive_summary = get_summary(model_dict, tokenizer_dict, extractive_summary_from_url)\n",
    "print(abstractive_summary)\n",
    "display(Markdown('### Sentiment analysys'))\n",
    "print(nlp_sentiment(abstractive_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer_nem = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model_nem = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "nlp = pipeline(\"ner\", model=model_nem, tokenizer=tokenizer_nem)\n",
    "ner_results = nlp(abstractive_summary)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('.venvML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ebb7f5bbb98572b15bff58c8aed07b0a474ef83fd7734f6b950989757680d4c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
